# Server
appName=LogConsumer
rbAppName=RabbitMQConsumer

# Kafka and 
batchDuration=10
metadata.broker.list=10.220.75.87:9092,10.220.75.87:9094,10.220.75.87:9095,10.220.75.87:9096
request.required.acks=1
zookeeper.connection.timeout.ms=10000
# Producer settings
serializer.class=com.adr.bigdata.dataimport.kafka.SolrInputDocumentEncoder

# Streaming Spark

sparkMaster=spark://10.220.75.79:7077
maxNumDStream=50
numStreams=2

# Topic and Partitions
topic=dataImport2
numPartitions=100
numParPerTopic=2

# Zookeeper 
kafka.zookeeper.connect=10.220.75.87:9093
zkConnectionTimeout=10000

# Group user
user=dataImporter

#SolrCloud params
solr.zookeeper.connect=10.220.75.80:9093,10.220.75.81:9093,10.220.75.82:9093
collection=commit_policy_product
solrBatchSize=500

# RabbitMQ
rabbitMQHost=10.220.75.133
rabbitMQPort=5672
rabbitMQQueueName=kafka_spark_queue